Introduction (page 3)
Pretraining (page 4)
  Data (page 4)
  Tokenization (page 6)
  Model (page 6)
    Architecture (page 6)
    Context Length Extension (page 7)
  Training (page 8)
  Experimental Results (page 8)
Alignment (page 9)
  Supervised Finetuning (page 9)
    Data (page 9)
    Training (page 10)
  Reinforcement Learning from Human Feedback (page 10)
    Reward Model (page 10)
    Reinforcement Learning (page 11)
  Automatic and Human Evaluation of Aligned Models (page 11)
  Tool Use, Code Interpreter, and Agent (page 13)
Code-Qwen: Specialized Model for Coding (page 16)
  Code Pretraining (page 16)
  Code Supervised Fine-Tuning (page 17)
  Evaluation (page 17)
Math-Qwen: Specialized Model for Mathematics Reasoning (page 17)
  Training (page 17)
  Evaluation (page 20)
Related Work (page 20)
  Large Language Models (page 20)
  Alignment (page 20)
  Tool Use and Agents (page 21)
  LLM for Coding (page 21)
  LLM for Mathematics (page 22)
Conclusion (page 22)
Appendix (page 35)
  More Training Details (page 35)
    Data Format for Qwen-Chat (page 35)
  Evaluation (page 35)
    Automatic Evaluation (page 35)
    Human Evaluation (page 40)
  Analysis of Code Interpreter (page 58)